---
title: "Performance Metrics in an 'Omics-Scanning Context"
output:
   github_document:
     pandoc_args: "--webtex"

bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```


```{r, echo=FALSE}
library(tidyverse)
library(caret)
library(patchwork)
library(viridis)

source("scripts/calc_cm_metrics.R")
set.seed(6)
```

# Background

There are many metrics that are typically reported when evaluating the performance of machine learning based AMP prediction methods (see for example [@Xu2021-ku] ).  At their core, all of these metrics are based on the four quantities that make up the "confusion matrix", and which represent modes of successful classification (TN, TP) and modes of failure (FP, FN):

mode | AMPs (+tve) | non-AMP (-ve)
-----|-------------|--------------
Correct | TP | TN
Incorrect | FP | FN

Where the abbreviations are as follows:

  - TP: True Positives
  - TN: True Negatives
  - FP: False Positives
  - FN: False Negatives

Typical metrics calculated from these four elements include Sn (Sensitivity), Sp (Specificity), Pr (Precision), Acc (Accuracy), MCC (Mathews Correlation Coefficient) with formulae for calculation as follows:

$$
Sn = \frac{TP}{TP+FN} \\
Sp = \frac{TN}{TN+FP} \\
Pr = \frac{TP}{TP+FP} \\
Acc = \frac{TP+TN}{TP+FN+TN+FP} \\
MCC = \frac{TP * TN - FP * FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}
$$


When reporting performance of a new predictor, or performing a methods review it is common to report some or all of these measures.  In such cases a single value per metric per predictor is typically reported, however, it is important to recognise that for a given predictor the values of the confusion matrix and associated performance metrics will depend on two additional key factors; 

1. The classification threshold, $p$.  Usually the probability calculated by the model that a sequence is an AMP
2. The balance of the test dataset, $\alpha$. This is the proportion of true AMP sequences in the data. 

There is a strong convention (adhered to in almost all papers) that these two values should be set as ($p=0.5, \alpha=0.5$) when calculating the single valued metrics mentioned above.  Another class of metrics measures the area under a curve, generated by varying the value of $p$ from 0 through to 1.  While these metrics (AUPRC, AUROC) do not explicitly choose $p=0.5$ they still attempt to reduce performance of the predictor to a single value across the range of $p$, placing equal emphasis on all parts of this range. 

In this section we show that these conventions on benchmarking are a poor fit for 'omics scanning applications, where two key considations come into play: 

1. The value of $\alpha$ is typically very small (ie AMPs are a minority class, typically on the order of 1% of sequences in the input). 
2. Obtaining a high value for precision $Pr$ is important, and in order to achieve this it is often necessary to work in a regime where $p$ is much higher than 0.5

# Effect of alpha on precision and recall curves

We begin by exploring how the two key characteristics of 'omics-scanning (low $\alpha$ and desire for high precision) interact.  To do this we explore how precision-recall curves change as a function of $\alpha$.  

First consider how the confusion matrix scales with $\alpha$.  This can be calculated by recognising that both TP and FN are actually AMPs and therefore scale with $\alpha$, whereas TN and FP are actually non-AMP and will therefore scale with $(1-\alpha)$.  Given a set of confusion matrix values generated with a balanced test set ($\alpha=0.5$) corresponding values for a different $\alpha$ can be calculated as follows:

$$
TP_{\alpha} = TP_{0.5}  \alpha \\
TN_{\alpha} = TN_{0.5}  (1-\alpha) \\
FP_{\alpha} = FP_{0.5}  (1-\alpha) \\
FN_{\alpha} = FN_{0.5}  \alpha 
$$

By extension the precision and recall can therefore be calculated as:

$$
Pr_{\alpha} = \frac{TP  \alpha}{TP  \alpha + FP  (1-\alpha)} \\
Re_{\alpha} = \frac{TP \alpha}{TP \alpha + FN \alpha} = \frac{TP}{TP  + FN }
$$
Note here that Recall, $Re$ is unaffected by $\alpha$ whereas precision will decrease as $\alpha$ decreases. Importantly, this effect on precision is stronger the higher the false positive rate. As a consequence of this, an 'omics-scanning application that desires high precision will most likely need to adjust the threshold probability, $p$ to reduce false positives.

To see how this works in practice consider the variation in precision and recall curves for ampir as a function of $\alpha$. These are plotted in Figure 2.1 and shows that as $\alpha$ gets smaller and smaller the precision curve shifts so that high values are only achieved for very high `p_threshold` values.

```{r}
# The ampir v0.1 model has a balanced test set which we use as example data 
#
ampir_v0.1 <- readRDS("data/ampir_0.1.0_data/svm_Radial98_final.rds")
features98Test <- readRDS("data/ampir_0.1.0_data//features98TestNov19.rds")

test_pred_prob <- predict(ampir_v0.1, features98Test, type = "prob")

ampir_prob_data <- test_pred_prob %>%
  add_column(Label = features98Test$Label) %>%
  rename(Neg = Bg) %>%
  rename(prob_AMP = Tg) %>%
  mutate(Label = ifelse(Label == "Tg", "Pos", "Neg"))

# Calculate metrics from real predicted data
# This is the baseline curve (alpha=0.5) that we then scale to produce curves for different alpha
#
ampir_roc_data <- map_df(seq(0.01, 0.99, 0.01), calc_cm_metrics, ampir_prob_data)
```


```{r}
# Using a confusion matrix generated for alpha=0.5 at various p values scale Pr and Re to a specified alpha
#
scale_precision_recall <- function(df,alpha) {
  df %>% 
  mutate(Recall = Recall) %>% 
  mutate(Precision = TP*alpha / (TP*alpha+FP*(1-alpha))) %>% 
  select(Recall,Precision,p_threshold)
}

pr_data <- c(0.01,0.05,0.1,0.5) %>% map_dfr( ~ scale_precision_recall(ampir_roc_data,.x) %>% add_column(alpha=.x))
```


```{r}
variable_names <- c("0.01" = "alpha: 0.01",
                    "0.05" = "alpha: 0.05",
                    "0.1" = "alpha: 0.10",
                    "0.5" = "alpha: 0.50")

pr_plot <- pr_data %>% gather("metric","value",-p_threshold,-alpha) %>% 
  ggplot(aes(x=p_threshold,y=value)) + 
    geom_line(aes(colour = metric, linetype = metric)) + facet_wrap(~alpha, labeller= as_labeller(variable_names)) +
    labs(x = "Probability threshold", y = "", colour = "", linetype = "") +
    scale_x_continuous(breaks=c(0, 0.50, 1.00)) +
    scale_y_continuous(breaks=c(0, 0.50, 1.00)) +
    scale_colour_manual(values = c("blueviolet", "forestgreen")) +
    theme(legend.position = "left",
          legend.text = element_text(angle = 90, hjust = 0.5),
          legend.margin = margin(0,0,0,8),
          legend.box.margin=margin(-10,-10,-10,-10),
          legend.key = element_rect(fill = "white"),
          legend.key.height = unit(2, "cm"),
          legend.box.spacing = unit(0.1, "cm"),
          panel.background = element_blank(),
          axis.line = element_line(colour = "grey"))+
    guides(colour = guide_legend(reverse=TRUE)) +
    guides(linetype = guide_legend(reverse=TRUE))
pr_plot
```

**Figure 2.1:** The precision and recall curves for four different values of $\alpha$.

Another way to look at the effect of alpha on precision and recall is to plot one against the other in a so called precision-recall curve.  In these curves the probability threshold does not appear explicitly but still controls the variation in precision and recall across their range of values. 
A useful way to think of precision is that it defines the "purity" of our predicted set of AMPs whereas the sensitivity or recall defines the "completeness" of the predicted AMP set. This shows that at $\alpha=0.5$ it is possible to maintain high precision even at relatively high values of recall but this is not the case as $\alpha$ decreases. At values of $\alpha$ that are realistic for 'omics-scanning (~1%) it is necessary to choose between precision or recall as it is no longer possible to achieve high values for both.

```{r, echo=FALSE}
pr_plot_combo <- pr_data %>% 
  mutate(alpha = as.factor(alpha)) %>% 
  ggplot(aes(x=Recall, y=Precision)) +
  geom_line(aes(colour = alpha, linetype = alpha)) + 
  scale_color_viridis(discrete = TRUE) +
  theme(legend.key = element_rect(fill = "white"),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "grey")) +
  labs(colour = "alpha", linetype = "alpha") +
  guides(colour = guide_legend(reverse=TRUE)) +
  guides(linetype = guide_legend(reverse=TRUE))

pr_plot_combo
```

**Figure 2.2:** A precision-recall curve depicting various alpha values that represent different proportions of AMPs in a genome


```{r}
combo_plot <- (pr_plot) / pr_plot_combo + plot_annotation(tag_levels = 'A')
ggsave(combo_plot,filename = "figures/alpha_gradient_and_variousalphas.png", height = 8, width = 7)
```

# Effect of alpha on ROC curves

Previous work has highlighted the fact that with unbalanced data, ROC curves are less informative of algorithm performance compared with precision-recall curves [@Davis2006-di]. This can easily be seen by considering the two axes in a ROC curve (y=TPR,x=FPR), and the way that these scale with $\alpha$.  The equations below clearly show that neither the x or y axes of a ROC curve are affected by changes in $\alpha$ which implies that both ROC curves themselves and associated metrics (AUROC) are completely invariant with the dataset balance.  In some situations this may be desirable, however, for omics-scanning where there is a strong requirement for high precision (which does vary with $\alpha$) ROC-based metrics can provide a misleading estimate of performance.

$$
ROC_{y} = TPR = \frac{TP}{TP+FN} \\

ROC_{y}^{\alpha} = \frac{\alpha TP}{\alpha TP + \alpha FN} = TPR = ROC_y \\

ROC_{x} = FPR = \frac{FP}{FP+TN} \\
ROC_{x}^{\alpha} = \frac{(1-\alpha)FP}{(1-\alpha)FP + (1-\alpha)TN} = FPR = ROC_{x}
$$

# Supplementary: Empirical test that the formulae for alpha scaling of the confusion matrix are correct

Here we perform a simple check to ensure that the formulae shown above for scaling of precision and recall under different values of alpha are correct.  We do this by randomly subsampling the raw outputs from ampir to produce a dataset with a specified proportion of true AMPs (ie a specified $\alpha$).  

This random subsample (with $\alpha=0.01$) is then used to calculate a confusion matrix over a range of probability thresholds as done above for $\alpha=0.5$. Since the total number of AMPs in this random subsample is very small it is necessary to take the average across many such random subsamples to obtain a smooth curve representative of the original (larger) dataset. The precision and recall curves obtained via this bootstrapping process appear almost identical to those obtained using the theoretical scaling.

```{r}
ampir_prob_data_bg <- ampir_prob_data[grep("Neg", ampir_prob_data$Label),]
ampir_prob_data_tg <- filter(ampir_prob_data, Label == "Pos")


samples_list <- replicate(n = 100, ampir_prob_data_tg %>% 
                             slice_sample(n=10) %>% 
                             rbind(ampir_prob_data_bg), simplify = F)

if (file.exists("cache/samples_list_metrics.rds")){
  samples_list_metrics <- read_rds("cache/samples_list_metrics.rds")
} else {

  samples_list_metrics <- lapply(seq_along(samples_list), function(i) {
                                 map_df(seq(0.01, 0.99, 0.01),
                                          calc_cm_metrics, 
                                          samples_list[[i]])})

    write_rds(samples_list_metrics,"cache/samples_list_metrics.rds")
  }

#Now I have a list with 100 dataframes in it that each contain metrics calculated with `calc_cm_metrics` from 100 dataframes that each contain 10 random AMPs and 996 background proteins.

#I want to average the precision and recall columns over the 0.01 - 0.99 p_threshold range.
#To do this I bind all the dataframes in the list together into one big dataframe using`bind_rows`. Then I use the `group_by` function to group the samples by the p_threshold and then use `summarise` to create two new columns that contain the calculated average of precision and recall and finally convert it into long format again.

samples_metrics <- bind_rows(samples_list_metrics, .id = "sample")

samples_metrics_averages <- samples_metrics %>% 
                group_by(p_threshold) %>% 
                summarise(Precision_average = mean(Precision), 
                          Recall_average = mean(Recall)) %>%
                pivot_longer(cols = c("Precision_average", "Recall_average"),
                               names_to = "metric",
                               values_to = "value") %>%
                mutate(value = coalesce(value, 1)) #replace 0.99 prob NaN value with 1
```


```{r, echo = FALSE}
average_plot1perc <- ggplot(filter(samples_metrics_averages, p_threshold >=0.5), aes(x=p_threshold, y=value)) +
  geom_line(aes(color = metric)) + 
  scale_x_continuous(breaks=c(0, 0.50, 0.60, 0.70, 0.80, 0.90, 1.00)) +
  scale_colour_manual(values = c("blueviolet", "forestgreen"),
                      labels = c("Precision", "Recall")) +
  labs(x = "", y = "", color = "") +
  theme(legend.position = c(0.23, 0.53),
        legend.key = element_rect(fill = "white"),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank(),
        panel.background = element_blank(),
        axis.line = element_line(colour = "grey")) +
  guides(color = guide_legend(reverse=TRUE)) 

pr_data_alpha1_long <- pr_data %>% gather("metric","value",-p_threshold,-alpha) %>% filter(alpha == 0.01)
alpha_001plot <- ggplot(filter(pr_data_alpha1_long, p_threshold >= 0.5), aes(x=p_threshold, y=value)) +  geom_line(aes(colour = metric)) + 
  labs(x = "Probability threshold", y = "", colour = "") +
  scale_colour_manual(values = c("blueviolet", "forestgreen")) +
  scale_x_continuous(breaks=c(0,0.50, 0.60, 0.70, 0.80, 0.90, 1.00)) +
  theme(legend.position = c(0.2, 0.5),
        panel.background = element_blank(),
        legend.key=element_blank(),
        axis.line = element_line(colour = "grey")) +
  guides(colour = guide_legend(reverse = TRUE))

average_plot1perc / alpha_001plot + plot_annotation(tag_levels = 'A')

average1percvsalpha1pc <- average_plot1perc / alpha_001plot + plot_annotation(tag_levels = 'A')

ggsave("figures/alpha_vs_average.png", average1percvsalpha1pc)
```

**Figure 2.3:** The precision and recall curves over a range of probability thresholds for $\alpha=0.01$ **A:** Obtained empirically via bootstrapping: and **B:** obtained using theoretical scaling of the confusion matrix.



